{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Recurrent Neural Network Quickstart\n",
    "\n",
    "The purpose of this notebook is to serve as a rapid introduction to recurrent neural networks. All of the details can be found in `Deep Dive into Recurrent Neural Networks` while this notebook focuses on using the pre-trained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from IPython.display import HTML\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category = RuntimeWarning)\n",
    "warnings.filterwarnings('ignore', category = UserWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import get_data, generate_output, guess_human, seed_sequence, get_embeddings, find_closest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Training Data\n",
    "\n",
    "* Using patent abstracts from patent search for neural network\n",
    "* 3000+ patents total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patent_abstract</th>\n",
       "      <th>patent_date</th>\n",
       "      <th>patent_number</th>\n",
       "      <th>patent_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\" A \"\"Barometer\"\" Neuron enhances stability in...</td>\n",
       "      <td>1996-07-09</td>\n",
       "      <td>5535303</td>\n",
       "      <td>\"\"\"Barometer\"\" neuron for a neural network\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\" This invention is a novel high-speed neural ...</td>\n",
       "      <td>1993-10-19</td>\n",
       "      <td>5255349</td>\n",
       "      <td>\"Electronic neural network for solving \"\"trave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An optical information processor for use as a ...</td>\n",
       "      <td>1995-01-17</td>\n",
       "      <td>5383042</td>\n",
       "      <td>3 layer liquid crystal neural network with out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A method and system for intelligent control of...</td>\n",
       "      <td>2001-01-02</td>\n",
       "      <td>6169981</td>\n",
       "      <td>3-brain architecture for an intelligent decisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A method and system for intelligent control of...</td>\n",
       "      <td>2003-06-17</td>\n",
       "      <td>6581048</td>\n",
       "      <td>3-brain architecture for an intelligent decisi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     patent_abstract patent_date  \\\n",
       "0  \" A \"\"Barometer\"\" Neuron enhances stability in...  1996-07-09   \n",
       "1  \" This invention is a novel high-speed neural ...  1993-10-19   \n",
       "2  An optical information processor for use as a ...  1995-01-17   \n",
       "3  A method and system for intelligent control of...  2001-01-02   \n",
       "4  A method and system for intelligent control of...  2003-06-17   \n",
       "\n",
       "  patent_number                                       patent_title  \n",
       "0       5535303        \"\"\"Barometer\"\" neuron for a neural network\"  \n",
       "1       5255349  \"Electronic neural network for solving \"\"trave...  \n",
       "2       5383042  3 layer liquid crystal neural network with out...  \n",
       "3       6169981  3-brain architecture for an intelligent decisi...  \n",
       "4       6581048  3-brain architecture for an intelligent decisi...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../data/neural_network_patent_query.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 16192 unique words.\n",
      "There are 318563 sequences.\n"
     ]
    }
   ],
   "source": [
    "training_dict, word_idx, idx_word, sequences = get_data('../data/neural_network_patent_query.csv', training_len = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sequences of text are represented as integers\n",
    "    * `word_idx` maps words to integers\n",
    "    * `idx_word` maps integers to words\n",
    "* Features are integer sequences of length 50\n",
    "* Label is next word in sequence\n",
    "* Labels are one-hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  117,     7,   141,   277,     4,    18,    81,   110,    10,\n",
       "          219,    29,     1,   952,  2453,    19,     5,     6,     1,\n",
       "          117,    10,   182,  2166,    21,     1,    81,   178,     4,\n",
       "           13,   117,   894,    14,  6163,     7,   302,     1,     9,\n",
       "            8,    29,    33,    23,    74,   428,     7,   692,     1,\n",
       "           81,   183,     4,    13,   117],\n",
       "       [    6,    41,     2,    87,     3,  1340,    79,     7,     1,\n",
       "          409,   543,    22,   484,     6,     2,  2113,   728,    24,\n",
       "            1,   178,     3,     1,  1820,    55,    14, 13942,  7240,\n",
       "          244,     5,    14, 13943,  7240,   244,     5,     2,  2113,\n",
       "         7240,   244,     5,     2,    38,  9292,   244,     2,    49,\n",
       "         9292,   244,    14,    22, 13944]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dict['X_train'][:2]\n",
    "training_dict['y_train'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: user to provide samples . A recognition operation is performed on the user's handwritten input , and the user is not satisfied with the recognition result . The user selects an option to train the neural network on one or more characters to improve the recognition results . The user\n",
      "\n",
      "Label: is\n",
      "\n",
      "Features: and includes a number of amplifiers corresponding to the N bit output sum and a carry generation from the result of the adding process an augend input-synapse group , an addend input-synapse group , a carry input-synapse group , a first bias-synapse group a second bias-synapse group an output feedback-synapse\n",
      "\n",
      "Label: group\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, sequence in enumerate(training_dict['X_train'][:2]):\n",
    "    text = []\n",
    "    for idx in sequence:\n",
    "        text.append(idx_word[idx])\n",
    "        \n",
    "    print('Features: ' + ' '.join(text) + '\\n')\n",
    "    print('Label: ' + idx_word[np.argmax(training_dict['y_train'][i])] + '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Recurrent Neural Network\n",
    "\n",
    "* Embedding dimension = 100\n",
    "* 64 LSTM cells in one layer\n",
    "    * Dropout and recurrent dropout for regularization\n",
    "* Fully connected layer with 64 units on top of LSTM\n",
    "     * 'relu' activation\n",
    "* Drop out for regularization\n",
    "* Output layer produces prediction for each word\n",
    "    * 'softmax' activation\n",
    "* Adam optimizer with defaults\n",
    "* Categorical cross entropy loss\n",
    "* Monitor accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout, Embedding, Masking, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 100)         1619200   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                42240     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16192)             534336    \n",
      "=================================================================\n",
      "Total params: 2,197,856\n",
      "Trainable params: 2,197,856\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Embedding layer\n",
    "model.add(\n",
    "    Embedding(\n",
    "        input_dim=len(word_idx) + 1,\n",
    "        output_dim=100,\n",
    "        weights=None,\n",
    "        trainable=True))\n",
    "\n",
    "# Recurrent layer\n",
    "model.add(\n",
    "    LSTM(\n",
    "        64, return_sequences=False, dropout=0.1,\n",
    "        recurrent_dropout=0.1))\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Dropout for regularization\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(len(word_idx) + 1, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Pre-Trained Model\n",
    "\n",
    "Rather than waiting several hours to train the model, we can load in a model trained for 150 epochs. We'll demonstrate how to train this model for another 5 epochs which shouldn't take too long depending on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 222994 samples, validate on 95569 samples\n",
      "Epoch 1/5\n",
      "222994/222994 [==============================] - 50s 223us/step - loss: 3.7555 - acc: 0.2943 - val_loss: 4.7447 - val_acc: 0.2672\n",
      "Epoch 2/5\n",
      "222994/222994 [==============================] - 49s 218us/step - loss: 3.7455 - acc: 0.2961 - val_loss: 4.7355 - val_acc: 0.2679\n",
      "Epoch 3/5\n",
      "222994/222994 [==============================] - 49s 218us/step - loss: 3.7317 - acc: 0.2963 - val_loss: 4.7395 - val_acc: 0.2691\n",
      "Epoch 4/5\n",
      "222994/222994 [==============================] - 49s 218us/step - loss: 3.7209 - acc: 0.2966 - val_loss: 4.7410 - val_acc: 0.2692\n",
      "Epoch 5/5\n",
      "222994/222994 [==============================] - 49s 218us/step - loss: 3.7091 - acc: 0.2979 - val_loss: 4.7367 - val_acc: 0.2707\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Load in model and demonstrate training\n",
    "model = load_model('../models/train-embeddings-rnn.h5')\n",
    "h = model.fit(training_dict['X_train'], training_dict['y_train'], epochs = 5, batch_size = 2048, \n",
    "          validation_data = (training_dict['X_valid'], training_dict['y_valid']), \n",
    "          verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance: Log Loss and Accuracy on training data\n",
      "222994/222994 [==============================] - 24s 109us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.282083551313851, 0.33844408377189383]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance: Log Loss and Accuracy on validation data\n",
      "95569/95569 [==============================] - 10s 108us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.737925765920241, 0.2671891513580688]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = load_model('../models/train-embeddings-rnn.h5')\n",
    "print('Model Performance: Log Loss and Accuracy on training data')\n",
    "model.evaluate(training_dict['X_train'], training_dict['y_train'], batch_size = 2048)\n",
    "\n",
    "print('\\nModel Performance: Log Loss and Accuracy on validation data')\n",
    "model.evaluate(training_dict['X_valid'], training_dict['y_valid'], batch_size = 2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a minor amount of overfitting on the training data but it's not major. Using regularization in both the LSTM layer and after the fully dense layer can help to combat the prevalent issue of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Output\n",
    "\n",
    "We can use the fully trained model to generate output by starting it off with a seed sequence. The `diversity` controls the amount of stochasticity in the predictions: the next word predicted is selected based on the probabilities of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1 style=\"color: darkblue;\"><center>Seed Sequence</center></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\">obtaining empirical data of parameters for an existing integrated circuit manufacturing process and extrapolating the known data to a new technology to assess potential yields of the new technology from the known process. Further, process variables of the new process may be adjusted based upon the empirical data</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<h1 style=\"color: darkred;\"><center>RNN Generated</center></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\">< --- > based on the object. Therefore, the invention is a simple controller. In addition, a computational system is provided with other independent data. The method also</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<h1 style=\"color: darkgreen;\"><center>Actual</center></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\">< --- > in order to optimize the yields of the new technology. A logic based computing system such as a fuzzy logic or neural-network system may be utilized. The computing</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in generate_output(model, sequences, idx_word, seed_length = 50, new_words = 30, diversity = 0.75):\n",
    "    HTML(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1 style=\"color: darkblue;\"><center>Seed Sequence</center></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\">circuit configuration and compact circuit size. With the presently cutting-edge technology (0.15 Î¼m CMOS), approximately 1G synapses can be integrated on one chip. Accordingly, it is</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<h1 style=\"color: darkred;\"><center>RNN Generated</center></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\">< --- > trained automatically close an inner cell universal size thus reduces component weights as signals by known training technique to parameters close exposures and predefined instructions the data activation. The</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<h1 style=\"color: darkgreen;\"><center>Actual</center></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\">< --- > possible to implement a neural network with approximately 30,000 neurons all coupled together on one chip. This corresponds to a network scale capable of associatively storing approximately 5,000 patterns</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in generate_output(model, sequences, idx_word, seed_length = 30, new_words = 30, diversity = 1.5):\n",
    "    HTML(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too high of a diversity and the output will be nearly random. Too low of a diversity and the model can get stuck outputting loops of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the network with own input\n",
    "\n",
    "Here you can input your own starting sequence for the network. The network will produce `num_words` of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1 style=\"color: black;\"><p><center>Input Seed <span style=\"color: red\">Network Output</center></p></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\"> <p>This patent provides a basis for using a recurrent neural network to <span style=\"color: red\">simple and automobiles, or or type or key computer to guide the original image. The neural network can</p></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'This patent provides a basis for using a recurrent neural network to '\n",
    "HTML(seed_sequence(model, s, word_idx, idx_word, diversity = 0.75, num_words = 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1 style=\"color: black;\"><p><center>Input Seed <span style=\"color: red\">Network Output</center></p></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\"> <p>The cell state is passed along from one time step to another allowing the <span style=\"color: red\">group of data to emulate a pre-determined model neural network. The neural network includes a set of nodes to</p></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'The cell state is passed along from one time step to another allowing the '\n",
    "HTML(seed_sequence(model, s, word_idx, idx_word, diversity = 0.75, num_words = 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guess if Output is from network or human\n",
    "\n",
    "The next function plays a simple game: is the output from a human or the network? Two of the choices are computer generated while the third is the actual ending but the order is randomized. Try to see if you can discern the differences! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed Sequence: unit includes a capacitor for storing a synapse load value information in a form of electric charges, and a refresh control circuit for remedying the change in the amount of the electric charges stored in the capacitor. The refresh control circuit includes a comparator for comparing a potential\n",
      "\n",
      "\n",
      "Option 1 < --- > at the neuron. A signal in the weight values is calculated during a first insulating voltage signal, and a first phototransistor on the liquid crystal sandwiched. The processing unit of the transistor is a threshold value. The\n",
      "\n",
      "Option 2 < --- > on a random amount of the temperature and a circuit. The charging the output voltage is applied so that the electrical output of the photosensitive body is obtained by the potential of the floating gate, and the circuit for\n",
      "\n",
      "Option 3 < --- > at an electrode of the capacitor and a reference potential, and a drive circuit responsive to the comparator for recovering the electric charges of the capacitor through charge pumping operation. The synapse load value information is refreshed, and\n",
      "\n",
      "\n",
      "Enter option you think is human (1-3): 2\n",
      "\n",
      "\n",
      "***Incorrect***\n",
      "\n",
      "------------------------------------------------------------\n",
      "Correct Ordering:  ['computer0', 'computer1', 'human']\n",
      "Diversity 0.81\n"
     ]
    }
   ],
   "source": [
    "guess_human(model, sequences, idx_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed Sequence: ##EQU1## where.alpha. is the coefficient, u.sub.(i) is the value which results from converting the input analog video image into the binary value, and P.sub.(i,j) is the value obtained from the function and g.sub.(i) is the value which is obtained\n",
      "\n",
      "\n",
      "Option 1 < --- > from the function and the input analog video image, it is possible to convert the video image into the binary value\n",
      "\n",
      "Option 2 < --- > between the premise of the premise of the neural network to generate a gain of the input data. Additionally, an\n",
      "\n",
      "Option 3 < --- > of the output of the neural network together, performing a logic unit processing device. The output for the control region\n",
      "\n",
      "\n",
      "Enter option you think is human (1-3): 2\n",
      "\n",
      "\n",
      "***Incorrect***\n",
      "\n",
      "------------------------------------------------------------\n",
      "Correct Ordering:  ['human', 'computer0', 'computer1']\n",
      "Diversity 0.95\n"
     ]
    }
   ],
   "source": [
    "guess_human(model, sequences, idx_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed Sequence: field effect transistors of different widths, by subtracting a plurality of differential signal components from an opposite most significant component, or by subtracting one half of a differential signal component from the opposite next most significant component. The neuron may provide binary sign selection and digit selection\n",
      "\n",
      "\n",
      "Option 1 < --- > signal using a discrete learning period of distinguishing signals.\n",
      "\n",
      "Option 2 < --- > of a drain to active discharge, such as a\n",
      "\n",
      "Option 3 < --- > by switching input and reference signals at each synapse.\n",
      "\n",
      "\n",
      "Enter option you think is human (1-3): 3\n",
      "\n",
      "\n",
      "***Correct***\n",
      "\n",
      "------------------------------------------------------------\n",
      "Ordering:  ['computer0', 'computer1', 'human']\n",
      "Diversity 0.94\n"
     ]
    }
   ],
   "source": [
    "guess_human(model, sequences, idx_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Embeddings\n",
    "\n",
    "As a final piece of model inspection, we can look at the embeddings and find the words closest to a query word in the embedding space. This gives us an idea of what the network has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16192, 100)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = get_embeddings(model)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word in the vocabulary is now represented as a 100-dimensional vector. This could be reduced to 2 or 3 dimensions for visualization. It can also be used to find the closest word to a query word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: network\n",
      "\n",
      "Word: network         Cosine Similarity: 1.0\n",
      "Word: channel         Cosine Similarity: 0.7754999995231628\n",
      "Word: networks        Cosine Similarity: 0.7745000123977661\n",
      "Word: system          Cosine Similarity: 0.7559999823570251\n",
      "Word: program         Cosine Similarity: 0.7541999816894531\n",
      "Word: cable           Cosine Similarity: 0.7419999837875366\n",
      "Word: now             Cosine Similarity: 0.7297999858856201\n",
      "Word: programming     Cosine Similarity: 0.7179999947547913\n",
      "Word: web             Cosine Similarity: 0.7138000130653381\n",
      "Word: line            Cosine Similarity: 0.6915000081062317\n"
     ]
    }
   ],
   "source": [
    "find_closest('network', embeddings, word_idx, idx_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A word should have a cosine similarity of 1.0 with itself! The embeddings are learned for a task, so the nearest words may only make sense in the context of the patents on which we trained the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: data\n",
      "\n",
      "Word: data            Cosine Similarity: 1.0\n",
      "Word: information     Cosine Similarity: 0.8185999989509583\n",
      "Word: numbers         Cosine Similarity: 0.683899998664856\n",
      "Word: database        Cosine Similarity: 0.6776000261306763\n",
      "Word: account         Cosine Similarity: 0.6575999855995178\n",
      "Word: report          Cosine Similarity: 0.6575999855995178\n",
      "Word: signals         Cosine Similarity: 0.6399999856948853\n",
      "Word: system          Cosine Similarity: 0.6377000212669373\n",
      "Word: statistics      Cosine Similarity: 0.6371999979019165\n",
      "Word: web             Cosine Similarity: 0.6359000205993652\n"
     ]
    }
   ],
   "source": [
    "find_closest('data', embeddings, word_idx, idx_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the network has learned some basic relationships between words! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "In this notebook, we saw a rapid introduction to recurrent neural networks. The full details can be found in `Deep Dive into Recurrent Neural Networks`. Recurrent neural networks are a powerful tool for natural language processing because of their ability to keep in mind an entire input sequence as they process one word at a time. This makes them applicable to sequence learning tasks where the order of the inputs matter and there can be long-term dependencies in the input sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
